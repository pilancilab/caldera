{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phantominator import shepp_logan\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from loguru import logger\n",
    "import pathlib\n",
    "from natsort import natsorted, ns\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes 01/18\n",
    "- Hadamard transform during finetuning\n",
    "- Change the optimization scheme for LoftQ --- try a projected ADMM algorithm (usually does better than just alternating optimization)\n",
    "- Q + LR + Sparse, optimize via ADMM. Make sparsity a constraint instead of an $\\ell_1$ term, with a pre-determined sparsity level\n",
    "\n",
    "For quant: $$min_{Q \\in \\mathcal{C}, Q' \\in \\mathbb{R}^{n\\times m}, L, R} \\text{<fro norm>} \\text{ s.t. } Q'=Q$$\n",
    "\n",
    "- Use the spectrum to determine which scheme to use\n",
    "\n",
    "### Notes 01/11\n",
    "- Preventing Diverging LoftQ optimization w/ \"momentum\":\n",
    "$B_{k+1} = Q((1-\\alpha) B_k + \\alpha * B_k)$\n",
    "- Question: does NF work or not; convergence issue\n",
    "- Try sketching\n",
    "- Can rotate $L$ and $R$ with some unitary matrix $H$ that makes the data more NF-quantizable (more Gaussian)\n",
    "- Add F-norm regularization on $L$, $R$\n",
    "- Some normalize and shift\n",
    "- Sparse $Q$\n",
    "- Make $Q$ have a Kronecker structure ($\\exist$ papers on this)\n",
    "- Replace $Q$ with $QD$, where $D$ is full-precision and diagonal\n",
    "- Try changing the objective function (type of norm)?\n",
    "- Use data-aware oprimization: right-multiply by a batch of training data (see: GPTQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lplr_llm.quantization import *\n",
    "from lplr_llm.weight_compressors import *\n",
    "from lplr_llm.hyperparameter_sweeps import *\n",
    "from lplr_llm.benchmarkers import *\n",
    "from peft.utils.loftq_utils import loftq_init\n",
    "from peft.utils.loftq_lplr_utils import loftq_lplr_init\n",
    "from peft.utils.quantization_utils import NFQuantizerFactory\n",
    "from lplr_llm.enums import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DEVICE = \"cuda:2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print names of all layers in mistral\n",
    "for layer_name, layer_weight in mistral.named_parameters():\n",
    "    print(f\"Layer name: {layer_name}: Shape:{layer_weight.detach().to(DEFAULT_DEVICE).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name, X_mis = list(mistral.named_parameters())[10]\n",
    "X_mis = X_mis.detach().to(DEFAULT_DEVICE)\n",
    "print(X_mis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = np.quantile(X_mis.flatten().cpu(), np.arange(200)/200)\n",
    "approx_pdf = 2/(quantiles[20:-10] -quantiles[10:-20])\n",
    "approx_pdf_idxs = quantiles[15:-15]\n",
    "plt.figure(figsize=(15, 3))\n",
    "plt.title(\"Approximate distribution of Weights\")\n",
    "plt.xlabel(\"Weight Value\")\n",
    "plt.ylabel(\"PDF\")\n",
    "plt.plot(approx_pdf_idxs, approx_pdf, color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_mis.shape)\n",
    "_, S, _ = torch.linalg.svd(X_mis.float(), full_matrices=False)\n",
    "\n",
    "# Plot the singular values\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(S.cpu(), marker='o', linestyle='-', color='b')\n",
    "plt.title(f'Singular Values of Mistral layer {layer_name}')\n",
    "plt.xlabel('Index', color=\"white\")\n",
    "plt.ylabel('Singular Value', color=\"white\")\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a phantominator matrix\n",
    "# X = torch.Tensor(shepp_logan(2048))\n",
    "# plt.imshow(X, cmap=\"gray\", interpolation=\"nearest\")\n",
    "# plt.show()\n",
    "# _, S, _ = torch.linalg.svd(X.float(), full_matrices=False)\n",
    "\n",
    "# # Plot the singular values\n",
    "# plt.figure(figsize=(11, 3))\n",
    "# plt.plot(S, marker='o', linestyle='-', color='b')\n",
    "# plt.title('Singular Values of Phantom(1024)')\n",
    "# plt.xlabel('Index', color=\"white\")\n",
    "# plt.ylabel('Singular Value', color=\"white\")\n",
    "# plt.yscale('log')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_iterative_weight_compression(\n",
    "    X = None,\n",
    "    weight_comp_configs: list[WeightCompressionConfig]=[\n",
    "        WeightCompressionConfig(\n",
    "            algorithm_type=AlgorithmType.ALTERNATING_MIXED_LPLR,\n",
    "            algorithm_kwargs={\n",
    "                \"k\": 64, \"r1\": 0, \"r2\": 0,\n",
    "                \"B1\": 8, \"B2\": 8\n",
    "            }\n",
    "        )\n",
    "    ],\n",
    "    plot_title = \"Frobenius Norm Errors over Iterations\",\n",
    "    seed=42\n",
    "):\n",
    "    plot_colors = [\"b\", \"r\", \"g\", \"c\", \"m\", \"k\"]\n",
    "    plot_markers = [\"o\", \"X\", \"*\"]\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    plt.figure(figsize=(11, 3))\n",
    "    for i, config in enumerate(weight_comp_configs):\n",
    "        kwargs = config.algorithm_kwargs.copy()\n",
    "        algorithm_type = config.algorithm_type\n",
    "        kwargs[\"log_errors\"] = True\n",
    "\n",
    "        if config.hadamard:\n",
    "            result = hadamard_weight_compression(\n",
    "                X=X, config=WeightCompressionConfig(\n",
    "                    algorithm_type=config.algorithm_type,\n",
    "                    algorithm_kwargs=kwargs\n",
    "                )\n",
    "            )\n",
    "            errors = result[-1]\n",
    "        elif algorithm_type == AlgorithmType.ALTERNATING_MIXED_LPLR:\n",
    "            kwargs[\"X\"] = X\n",
    "            _, _, errors = alternating_mixed_lplr(**kwargs)\n",
    "        elif algorithm_type == AlgorithmType.DIRECT_SVD_LPLR:\n",
    "            kwargs[\"X\"] = X\n",
    "            _, _, errors = direct_svd_mixed_lplr(**kwargs)\n",
    "        elif algorithm_type == AlgorithmType.LOFTQ:\n",
    "            kwargs[\"weight\"] = X\n",
    "            _, _, _, errors = loftq_init(**kwargs)\n",
    "        else: ## Loftq-LPLR\n",
    "            kwargs[\"weight\"] = X\n",
    "            _, _, _, errors = loftq_lplr_init(**kwargs)\n",
    "\n",
    "        fro_norm_X = torch.norm(X, p=\"fro\").item()\n",
    "        relative_errors = np.array(errors) / fro_norm_X\n",
    "\n",
    "        print(relative_errors)\n",
    "\n",
    "        # Plot errors over iterations\n",
    "        plt.plot(\n",
    "            range(1, len(relative_errors) + 1),\n",
    "            relative_errors,\n",
    "            marker=plot_markers[(i // len(plot_colors)) % len(plot_markers)],\n",
    "            linestyle=\"-\",\n",
    "            markersize=4,\n",
    "            color=plot_colors[i % len(plot_colors)],\n",
    "            label=f\"Param Set {i+1}*\")\n",
    "\n",
    "    print(\"-\"*80, \"\\n* Legend Key\")\n",
    "    for i, config in enumerate(weight_comp_configs):\n",
    "        print(f\"Param Set {i+1}: \", config.algorithm_kwargs)\n",
    "        print(\"\\tusing algorithm type \", config.algorithm_type)\n",
    "        if config.hadamard:\n",
    "            print(\"\\twith randomized Hadamard transform\")\n",
    "\n",
    "    plt.title(plot_title)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WITHOUT HADAMARD SKETCH\n",
    "test_iterative_weight_compression(\n",
    "    weight_comp_configs=[\n",
    "        WeightCompressionConfig(\n",
    "            algorithm_kwargs={\n",
    "                \"num_bits\": 4, \"reduced_rank\": 64, \"num_iter\": 30, \"quantizer_factory\": QuantizerFactory(\"normal\")\n",
    "            },\n",
    "            algorithm_type=AlgorithmType.LOFTQ,\n",
    "            hadamard=False\n",
    "        ),\n",
    "        # WeightCompressionConfig(\n",
    "        #     algorithm_kwargs={\n",
    "        #         \"num_bits\": 4, \"num_bits_factors\": 8, \"reduced_rank\": 64,\n",
    "        #         \"num_iter\": 50, \"num_iter_lplr\": 30, \"quantizer_factory\": QuantizerFactory(\"normal\")\n",
    "        #     },\n",
    "        #     algorithm_type=AlgorithmType.LOFTQ_LPLR,\n",
    "        #     hadamard=False\n",
    "        # )\n",
    "    ],\n",
    "    plot_title=\"Frobenius Norm Errors over Iterations\",\n",
    "    X=X_mis.T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WITH HADAMARD\n",
    "test_iterative_weight_compression(\n",
    "    weight_comp_configs=[\n",
    "        WeightCompressionConfig(\n",
    "            algorithm_kwargs={\n",
    "                \"num_bits\": 4, \"reduced_rank\": 64, \"num_iter\": 50, \"quantizer_factory\": QuantizerFactory(\"normal\")\n",
    "            },\n",
    "            algorithm_type=AlgorithmType.LOFTQ,\n",
    "            hadamard=True\n",
    "        ),\n",
    "        # WeightCompressionConfig(\n",
    "        #     algorithm_kwargs={\n",
    "        #         \"num_bits\": 4, \"num_bits_factors\": 8, \"reduced_rank\": 64,\n",
    "        #         \"num_iter\": 50, \"num_iter_lplr\": 30, \"quantizer_factory\": QuantizerFactory(\"normal\")\n",
    "        #     },\n",
    "        #     algorithm_type=AlgorithmType.LOFTQ_LPLR,\n",
    "        #     hadamard=True\n",
    "        # )\n",
    "    ],\n",
    "    plot_title=\"Frobenius Norm Errors over Iterations\",\n",
    "    X=X_mis.T,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iterative_weight_compression(\n",
    "    weight_comp_configs=[\n",
    "        WeightCompressionConfig(\n",
    "            algorithm_kwargs={\n",
    "                \"num_bits\": 4, \"reduced_rank\": 64, \"num_iter\": 50, \"quantizer_factory\": QuantizerFactory(\"uniform_clipped\")\n",
    "            },\n",
    "            algorithm_type=AlgorithmType.LOFTQ,\n",
    "            hadamard=True\n",
    "        ),\n",
    "        WeightCompressionConfig(\n",
    "            algorithm_kwargs={\n",
    "                \"num_bits\": 4, \"reduced_rank\": 64, \"num_iter\": 50, \"quantizer_factory\": QuantizerFactory(\"uniform_clipped\")\n",
    "            },\n",
    "            algorithm_type=AlgorithmType.LOFTQ,\n",
    "            hadamard=False\n",
    "        )\n",
    "    ],\n",
    "    plot_title=\"Frobenius Norm Errors over Iterations\",\n",
    "    X=X_mis.T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do some hyperparameter sweeps on the Shepp-Logan matrix\n",
    "average_bit_level = 3\n",
    "budget = X_mis.shape[0] * X_mis.shape[1] * average_bit_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtxs, alpha, beta, B, error = lplr_sweep_alpha_and_B(\n",
    "    X=X_mis.T, budget=budget,\n",
    "    weight_comp_config=WeightCompressionConfig(\n",
    "        algorithm_kwargs={\"quantizer_factory\": NFQuantizerFactory(\"normal\"), \"iters\":30},\n",
    "        algorithm_type=AlgorithmType.ALTERNATING_MIXED_LPLR\n",
    "    ),\n",
    "    debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtxs, alpha, beta, B, error = lplr_sweep_alpha_and_B(\n",
    "    X=X_mis.T, budget=budget,\n",
    "    weight_comp_config=WeightCompressionConfig(\n",
    "        algorithm_kwargs={\"quantizer_factory\": QuantizerFactory(\"normal\"), \"num_bits\": 2, \"num_iter\": 20, \"num_iter_lplr\": 20},\n",
    "        algorithm_type=AlgorithmType.LOFTQ_LPLR\n",
    "    ),\n",
    "    debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenchmarkerComparison():\n",
    "    def __init__(\n",
    "        self,\n",
    "        benchmarkers: list[WeightCompressionBenchmarker] = [],\n",
    "        enforce_budget = True,\n",
    "        average_bit_level = 4,\n",
    "        save_to_csv = False,\n",
    "        continue_csv = False,\n",
    "        save_file = None,\n",
    "        reset_error_lists = True\n",
    "    ):\n",
    "        if reset_error_lists:\n",
    "            for benchmarker in benchmarkers:\n",
    "                benchmarker.reset_errors()\n",
    "\n",
    "        self.benchmarkers = benchmarkers\n",
    "        self.save_to_csv = save_to_csv or continue_csv\n",
    "        self.continue_csv = continue_csv\n",
    "        self.save_file = save_file\n",
    "        self.enforce_budget = enforce_budget\n",
    "        self.average_bit_level = average_bit_level if enforce_budget else 256\n",
    "        \n",
    "        if save_to_csv and not continue_csv:\n",
    "            with open(save_file, 'w') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "\n",
    "                first_headers = [\"Layer Name\", \"n\", \"d\"]\n",
    "                if enforce_budget:\n",
    "                    first_headers.append(\"Bit Budget\")\n",
    "                writer.writerow(first_headers + [benchmarker.label for benchmarker in benchmarkers])\n",
    "        if continue_csv:\n",
    "            df = pd.read_csv(save_file)\n",
    "            self.prev_layer_names = list(df[\"Layer Name\"])\n",
    "    \n",
    "    def write_latest_data(self, layer_name, n, d, budget=0):\n",
    "        with open(self.save_file, 'a') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            first_items = [layer_name, n, d]\n",
    "            if self.enforce_budget:\n",
    "                first_items.append(budget)\n",
    "            writer.writerow(first_items + [benchmarker.errors[-1] for benchmarker in self.benchmarkers])\n",
    "    \n",
    "    def run_on_matrix(self, layer_name, X):\n",
    "        if self.continue_csv and layer_name in self.prev_layer_names:\n",
    "            return\n",
    "        print(f\"Benchmarking {layer_name}\")\n",
    "        n, d = X.size()\n",
    "        budget = n*d*self.average_bit_level\n",
    "\n",
    "        for benchmarker in self.benchmarkers:\n",
    "            benchmarker.run(X, budget)\n",
    "\n",
    "        if self.save_to_csv:\n",
    "            self.write_latest_data(layer_name, n, d, budget)\n",
    "\n",
    "    def print_errors(self):\n",
    "        for benchmarker in self.benchmarkers:\n",
    "            print(f\"{benchmarker.label}: {benchmarker.errors}\")\n",
    "\n",
    "    def plot_errors(self):\n",
    "        plot_colors = [\"b\", \"r\", \"g\", \"c\", \"m\", \"k\"]\n",
    "        plot_markers = [\"o\", \"X\", \"*\"]\n",
    "\n",
    "        plt.figure(figsize=(15, 4))\n",
    "        for i, benchmarker in enumerate(self.benchmarkers):\n",
    "            plt.plot(\n",
    "                benchmarker.errors,\n",
    "                marker=plot_markers[(i // len(plot_colors)) % len(plot_markers)],\n",
    "                linestyle=\"-\",\n",
    "                markersize=4,\n",
    "                color=plot_colors[i % len(plot_colors)],\n",
    "                label=benchmarker.label\n",
    "            )\n",
    "        plt.title(\"Relative Frobenius Error\")\n",
    "        plt.xlabel(\"Layer\")\n",
    "        plt.ylabel(\"Error\")\n",
    "        plt.yscale(\"log\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenchmarkerComparisonList(BenchmarkerComparison):\n",
    "    def __init__(\n",
    "        self,\n",
    "        X_list: list[torch.Tensor] = [],\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.X_list = X_list\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def run(self):\n",
    "        for i, X in enumerate(self.X_list):\n",
    "            self.run_on_matrix(f\"Matrix {i}\", X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenchmarkerComparisonModel(BenchmarkerComparison):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        device = \"cpu\",\n",
    "        layer_limit:int = -1, # Limits the number of weight matrices used\n",
    "                              # for benchmarking (mainly for debugging purposes).\n",
    "                              # -1 means no limit.\n",
    "         max_num_cols:int = -1, # Excludes weight matrices that are too large.\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.layer_limit = layer_limit if layer_limit > 0 else float('inf')\n",
    "        self.max_num_cols = max_num_cols if max_num_cols > 0 else float('inf')\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def run(self):\n",
    "        layer_count = 0\n",
    "        for layer_name, X in self.model.named_parameters():\n",
    "            if self.continue_csv and layer_name in self.prev_layer_names:\n",
    "                continue\n",
    "            if X.dim() != 2:\n",
    "                continue\n",
    "            if X.shape[0] < X.shape[1]:\n",
    "                X = X.T\n",
    "            if X.shape[0] > self.max_num_cols:\n",
    "                logger.info(\"Layer larger than maximum size specified, skipping.\")\n",
    "                continue\n",
    "\n",
    "            layer_count += 1\n",
    "            if layer_count > self.layer_limit:\n",
    "                logger.info(\"Reached layer limit, exiting.\")\n",
    "                return layer_count\n",
    "            \n",
    "            self.run_on_matrix(layer_name, X.float().to(self.device))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarkers = [\n",
    "        # LoftqBenchmarker(\n",
    "        #     WeightCompressionConfig(\n",
    "        #         algorithm_kwargs={\n",
    "        #             \"num_iter\": 20,\n",
    "        #             \"reduced_rank\": 64,\n",
    "        #             \"num_bits\": 2,\n",
    "        #             \"quant_type\": QuantType.UNIFORM\n",
    "        #         },\n",
    "        #         algorithm_type=AlgorithmType.LOFTQ\n",
    "        #     ),\n",
    "        #     fixed_rank=True,\n",
    "        #     label=\"LoftQ (2b)\"\n",
    "        # ),\n",
    "        # LoftqBenchmarker(\n",
    "        #     WeightCompressionConfig(\n",
    "        #         algorithm_kwargs={\n",
    "        #             \"num_iter\": 20,\n",
    "        #             \"reduced_rank\": 64,\n",
    "        #             \"num_bits\": 4,\n",
    "        #             \"quant_type\": QuantType.UNIFORM\n",
    "        #         },\n",
    "        #         algorithm_type=AlgorithmType.LOFTQ\n",
    "        #     ),\n",
    "        #     fixed_rank=True,\n",
    "        #     label=\"LoftQ (4b)\"\n",
    "        # ),\n",
    "        LoftqBenchmarker(\n",
    "            WeightCompressionConfig(\n",
    "                algorithm_kwargs={\n",
    "                    \"num_iter\": 20,\n",
    "                    \"reduced_rank\": 64,\n",
    "                    \"num_bits\": 8,\n",
    "                    \"quantizer_factory\": QuantizerFactory(\"normal\")\n",
    "                },\n",
    "                algorithm_type=AlgorithmType.LOFTQ\n",
    "            ),\n",
    "            label=\"LoftQ (8b)\"\n",
    "        ),\n",
    "        # LplrBenchmarker(\n",
    "        #     WeightCompressionConfig(\n",
    "        #         algorithm_kwargs={\n",
    "        #             \"quant_type\": QuantType.UNIFORM,\n",
    "        #             \"num_bits\": 2,\n",
    "        #             \"num_bits_factors\": 8,\n",
    "        #             \"reduced_rank\": 64,\n",
    "        #             \"num_iter\": 20,\n",
    "        #             \"num_iter_lplr\": 30\n",
    "        #         },\n",
    "        #         algorithm_type=AlgorithmType.LOFTQ_LPLR\n",
    "        #     ),\n",
    "        #     label=\"Loftq-LPLR (2b)\",\n",
    "        #     run_hyper_parameter_sweep=False\n",
    "        # ),\n",
    "        # LplrBenchmarker(\n",
    "        #     WeightCompressionConfig(\n",
    "        #         algorithm_kwargs={\n",
    "        #             \"quant_type\": QuantType.UNIFORM,\n",
    "        #             \"num_bits\": 4,\n",
    "        #             \"num_bits_factors\": 8,\n",
    "        #             \"reduced_rank\": 64,\n",
    "        #             \"num_iter\": 20,\n",
    "        #             \"num_iter_lplr\": 30\n",
    "        #         },\n",
    "        #         algorithm_type=AlgorithmType.LOFTQ_LPLR\n",
    "        #     ),\n",
    "        #     label=\"Loftq-LPLR (4b)\",\n",
    "        #     run_hyper_parameter_sweep=False\n",
    "        # ),\n",
    "        LplrBenchmarker(\n",
    "            WeightCompressionConfig(\n",
    "                algorithm_kwargs={\n",
    "                    \"quantizer_factory\": QuantizerFactory(\"normal\"),\n",
    "                    \"num_bits\": 8,\n",
    "                    \"num_bits_factors\": 8,\n",
    "                    \"reduced_rank\": 64,\n",
    "                    \"num_iter\": 20,\n",
    "                    \"num_iter_lplr\": 30\n",
    "                },\n",
    "                algorithm_type=AlgorithmType.LOFTQ_LPLR\n",
    "            ),\n",
    "            label=\"Loftq-LPLR (8b)\",\n",
    "            run_hyper_parameter_sweep=False\n",
    "        ),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For debugging, we set limits on how many layers we use\n",
    "## and their sizes\n",
    "comparison_object = BenchmarkerComparisonModel(\n",
    "    model=mistral,\n",
    "    device=DEFAULT_DEVICE,\n",
    "    layer_limit=5,\n",
    "    max_num_cols=15000,\n",
    "    benchmarkers=benchmarkers,\n",
    "    enforce_budget=False,\n",
    "    continue_csv=True,\n",
    "    save_file=\"/home/nsagan/experiments/results/Loftq_LPLR_comparsion_2.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_object.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
