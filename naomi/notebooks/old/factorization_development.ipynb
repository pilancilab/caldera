{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a very messy notebook for current development/scratch work.\n",
    "\n",
    "#### `test_decomposition_schemes.ipynb` has more formalized progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phantominator import shepp_logan\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "import torch_dct as dct\n",
    "import pywt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lplr_llm.experimental import make_sparse\n",
    "from lplr_llm.quantization import QuantizerFactory, simulated_quant\n",
    "from lplr_llm.error_metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DEVICE = \"cuda:2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name, X_lla = list(llama.named_parameters())[100]\n",
    "X_lla = X_lla.detach().to(DEFAULT_DEVICE)\n",
    "n, d = X_lla.shape\n",
    "layer_name, n, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUR Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cur_decomposition(X, c, r, k=None):\n",
    "    if k is None:\n",
    "        k = min(*X.shape)\n",
    "\n",
    "    U, _, VT = torch.linalg.svd(X, full_matrices=False)\n",
    "\n",
    "    p_col = 1/k * torch.norm(VT.T[:, :k], dim=1) ** 2\n",
    "    p_row =  1/k * torch.norm(U[:, :k], dim=1) ** 2\n",
    "\n",
    "    keep_rows = torch.rand(X.shape[0]).to(X.device) < r*p_row\n",
    "    while torch.sum(keep_rows) < r:\n",
    "        diff = r - torch.sum(keep_rows)\n",
    "        keep_rows = keep_rows | (torch.rand(X.shape[0]).to(X.device) < diff*p_row)\n",
    "    while torch.sum(keep_rows) > r:\n",
    "        keep_rows[torch.randint(X.shape[0], size=(1,))] = False\n",
    "\n",
    "    keep_cols = torch.rand(X.shape[1]).to(X.device) < c*p_col\n",
    "    while torch.sum(keep_cols) < c:\n",
    "        diff = c - torch.sum(keep_cols)\n",
    "        keep_cols = keep_cols | (torch.rand(X.shape[1]).to(X.device) < diff*p_col)\n",
    "    while torch.sum(keep_cols) > c:\n",
    "        keep_cols[torch.randint(X.shape[1], size=(1,))] = False\n",
    "\n",
    "    C = X[:, keep_cols]\n",
    "    R = X[keep_rows, :]\n",
    "\n",
    "    U = torch.linalg.pinv(C) @ X @ torch.linalg.pinv(R)\n",
    "\n",
    "    return C, U, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = np.log(k / 0.04) # log(k/epsilon^2) oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k*oversample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C, U, R = cur_decomposition(X_lla, int(k*oversample), int(k*oversample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hat_CUR = C @ U @ R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(X_lla - X_hat_CUR) / torch.norm(X_lla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, Sigma, VT = torch.linalg.svd(X_lla, full_matrices=False)\n",
    "X_hat_SVD = U[:, :k] @ torch.diag(Sigma[:k]) @ VT[:k, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(X_lla - X_hat_SVD) / torch.norm(X_lla)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The necessary oversampling to get the same accuracy seems prohibitive, but https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9335842&tag=1 gets good results using CUR so maybe the decrease in Frobenius norm accuracy at the same bitrate doesn't degrade finetuning performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pruning_matrix_approximation(X, block_size=4, n_segments=4):\n",
    "#     X_orig = X\n",
    "#     segment_length = X.shape[1] // n_segments\n",
    "\n",
    "#     for segment_start in range(0, X.shape[1], segment_length):\n",
    "\n",
    "#         errors = []\n",
    "#         for col in tqdm(range(\n",
    "#             segment_start, \n",
    "#             min(segment_start+segment_length, X.shape[1]),\n",
    "#             block_size\n",
    "#         )):\n",
    "#             X_minus_col = torch.hstack((X[:, :col], X[:, col+block_size:]))\n",
    "#             w = torch.linalg.lstsq(X_minus_col, X[:, col:col+block_size])[0]\n",
    "#             err = torch.norm(X[:, col:col+block_size] - X_minus_col @ w) / \\\n",
    "#                 torch.norm(X[:, col:col+block_size])\n",
    "#             errors.append(err.item())\n",
    "#         col = np.argmin(errors)\n",
    "#         X_minus_col = torch.hstack((X[:, :col], X[:, col+block_size:]))\n",
    "#         w = torch.linalg.lstsq(X_minus_col, X[:, col:col+block_size])[0]\n",
    "#         X = torch.hstack((X[:, :col], X_minus_col @ w, X[:, col+block_size:]))\n",
    "#     return X, (torch.norm(X - X_orig) / torch.norm(X)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_lla\n",
    "col = 50\n",
    "step = 4\n",
    "d = X.shape[1]\n",
    "X1 = torch.hstack((X[:, :col], X[:, col+step:]))\n",
    "W1 = torch.linalg.lstsq(X1, X[:, col:col+step])[0]\n",
    "P1 = torch.zeros(d, d).to(X.device)\n",
    "P1[torch.arange(col), torch.arange(col)] = 1\n",
    "P1[torch.arange(col, d-step), torch.arange(col+step, d)] = 1\n",
    "P1[torch.arange(d-step, d), torch.arange(col, col+step)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hat = X1 @ torch.hstack((torch.eye(X1.shape[1]).to(X.device), W1)) @ P1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(X - X_hat) / torch.norm(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step = 32\n",
    "# segment_len = 32 * step\n",
    "# segments = X.shape[1] // segment_len\n",
    "# iters = segments * 23\n",
    "\n",
    "# cols_to_keep = torch.arange(4096) >= 0\n",
    "# for i, _ in enumerate(tqdm(range(iters))):\n",
    "#     errors = []\n",
    "\n",
    "#     if not torch.all(cols_to_keep[col:col+step]):\n",
    "#         errors.append(float('inf'))\n",
    "#         continue\n",
    "#     cols_to_keep_copy = cols_to_keep.clone()\n",
    "\n",
    "#     cols_to_keep_copy[col:col+step] = False\n",
    "#     X_minus_cols = X[:, cols_to_keep_copy]\n",
    "#     cols = X[:, ~cols_to_keep_copy]\n",
    "#     W = torch.linalg.lstsq(X_minus_cols, cols)[0]\n",
    "#     err = torch.norm(cols - X_minus_cols @ W) / torch.norm(cols)\n",
    "#     errors.append(err.item())\n",
    "    \n",
    "#     col = np.argmin(errors)*step + (i%segments)*segment_len\n",
    "#     # print(col, np.min(errors), sum(cols_to_keep).item())\n",
    "#     cols_to_keep[col:col+step] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name, X_lla = list(llama.named_parameters())[100]\n",
    "X_lla = X_lla.detach().to(DEFAULT_DEVICE)\n",
    "n, d = X_lla.shape\n",
    "layer_name, n, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n < d:\n",
    "    X_lla = X_lla.T\n",
    "    n, d = d, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, Sigma, VT = torch.linalg.svd(X_lla)\n",
    "Xk = U[:, :64] @ torch.diag(Sigma[:64]) @ VT[:64, :]\n",
    "X = X_lla - Xk\n",
    "# X = X_lla.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = (2944 + 63) // 64 * 64 ## Must be a multiple of 64 or block quantization doesn't work\n",
    "\n",
    "## For MLP wide matrix\n",
    "# m = (d - n + 2400 + 63) // 64 * 64\n",
    "## For MLP tall matrix\n",
    "# m = (2400 + 63) // 64 * 64\n",
    "\n",
    "cols_to_keep = torch.arange(X.shape[1]) >= 0\n",
    "_, _, VT = torch.linalg.svd(X)\n",
    "col_idxs = torch.topk(torch.norm(VT.T[:, -m:], dim=1).cpu(), m).indices\n",
    "\n",
    "cols_to_keep[col_idxs] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_minus_cols = X[:, cols_to_keep]\n",
    "cols = X[:, ~cols_to_keep]\n",
    "W = torch.linalg.lstsq(X_minus_cols, cols)[0]\n",
    "err = torch.norm(cols - X_minus_cols @ W) / torch.norm(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_8b = QuantizerFactory(\"normal\").get_quantizer(8)\n",
    "quant_4b = QuantizerFactory(\"normal\").get_quantizer(4)\n",
    "quant_2b = QuantizerFactory(\"uniform_clipped\").get_quantizer(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant = quant_4b\n",
    "bits = 4\n",
    "err_norm = FroError(X_lla)\n",
    "\n",
    "# X_left = X_minus_cols\n",
    "# X_right = W\n",
    "\n",
    "X_left = simulated_quant(quant, X_minus_cols)\n",
    "X_right = simulated_quant(quant, torch.linalg.lstsq(X_left, cols)[0])\n",
    "\n",
    "A = torch.hstack((torch.eye(X_right.shape[0]).to(X_right.device), X_right))\n",
    "B = torch.hstack((X_minus_cols, cols))\n",
    "best_err = err_norm.error(X_hat=X_left @ A, X_exact=B)\n",
    "best_factors = X_left, X_right\n",
    "for _ in tqdm(range(15)):\n",
    "    A = torch.hstack((torch.eye(X_right.shape[0]).to(X_right.device), X_right))\n",
    "    B = torch.hstack((X_minus_cols, cols))\n",
    "    X_left = simulated_quant(quant, torch.linalg.lstsq(A.T, B.T)[0].T)\n",
    "    X_right = simulated_quant(quant, torch.linalg.lstsq(X_left, cols)[0])\n",
    "\n",
    "    A = torch.hstack((torch.eye(X_right.shape[0]).to(X_right.device), X_right))\n",
    "    B = torch.hstack((X_minus_cols, cols))\n",
    "    err = err_norm.error(X_hat=X_left @ A, X_exact=B)\n",
    "    # print(err)\n",
    "    if err < best_err:\n",
    "        best_err = err\n",
    "        best_factors = X_left, X_right\n",
    "X_left, X_right = best_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2B Quant: %e bits\" % (X.shape[1] * X.shape[0] * 2))\n",
    "print(\"4B Quant: %e bits\" % (X.shape[1] * X.shape[0] * 4))\n",
    "print(\"Ours: %e bits\" % (X_minus_cols.shape[0]*X_minus_cols.shape[1]*bits + W.shape[0]*W.shape[1]*bits  + X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_left A \\approx B\n",
    "err_norm = SpectralError(X_lla)\n",
    "\n",
    "A = torch.hstack((torch.eye(X_right.shape[0]).to(X_right.device), X_right))\n",
    "B = torch.hstack((X_minus_cols, cols))\n",
    "err = err_norm.error(X_hat=X_left @ A, X_exact=B)\n",
    "err_2b = err_norm.error(X_hat=simulated_quant(quant_2b, X), X_exact=X)\n",
    "err_4b = err_norm.error(X_hat=simulated_quant(quant_4b, X), X_exact=X)\n",
    "\n",
    "print(f\"2B Quant error: {(err_2b)}\")\n",
    "print(f\"4B Quant error: {(err_4b)}\")\n",
    "print(f\"Our error: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Sparse + Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shrinkage(tau, x):\n",
    "    return torch.sign(x) * torch.maximum(torch.abs(x) - tau, torch.zeros_like(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_plus_quantized(\n",
    "    X,\n",
    "    lmbda,\n",
    "    B=4,\n",
    "    BS=8,\n",
    "    iters=10\n",
    "):\n",
    "    quant = QuantizerFactory(\"normal\").get_quantizer(B)\n",
    "    quant_S = QuantizerFactory(\"normal\").get_quantizer(BS)\n",
    "\n",
    "    Q = simulated_quant(quant, X)\n",
    "    S = torch.zeros_like(X)\n",
    "    for _ in range(iters):\n",
    "        Q = simulated_quant(quant, X - S)\n",
    "        S = shrinkage(lmbda, X - Q)\n",
    "        if BS < 16:\n",
    "            S = simulated_quant(quant_S, S)\n",
    "\n",
    "        # enforce actual sparsity\n",
    "        S = S * (torch.abs(S) > torch.abs(X).max() * 1e-2)\n",
    "        print(f\"Fro error: {(torch.norm(X - Q - S) / torch.norm(X)).item()}\")\n",
    "        print(f\"\\tNonzero: {torch.sum(S != 0).item() / (4096*4096) * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_plus_quantized(X_lla, 1e-3, B=4, BS=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(0.22*4096*4096*32 + 4096*4096*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4096*4096*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
