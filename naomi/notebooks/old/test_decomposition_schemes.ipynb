{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports; Loading in Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phantominator import shepp_logan\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "import torch_dct as dct\n",
    "import pywt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lplr_llm.quantization import *\n",
    "from lplr_llm.weight_compressors import *\n",
    "from peft.utils.loftq_utils import loftq_init\n",
    "from peft.utils.quantization_utils import NFQuantizerFactory\n",
    "from lplr_llm.enums import *\n",
    "from lplr_llm.error_metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DEVICE = \"cuda:2\"\n",
    "DEFAULT_ERROR_NORM = RandSpectralError(oversample=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mistral = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    token=\"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\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Values, Matrix Visualization, DCT/Wavelet \n",
    "\n",
    "Code to plot singular values of weight matrices, plot log magnitude of weights, and apply DCT/Wavelet to weight matrices. So far, this exploration has not yielded anything helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_name, X_mis = list(mistral.named_parameters())[11]\n",
    "# X_mis = X_mis.detach().to(DEFAULT_DEVICE)\n",
    "# n, d = X_mis.shape\n",
    "# n, d\n",
    "layer_name, X_lla = list(llama.named_parameters())[100]\n",
    "X_lla = X_lla.detach().to(DEFAULT_DEVICE)\n",
    "n, d = X_lla.shape\n",
    "n, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, S, _ = torch.linalg.svd(X_lla.float(), full_matrices=False)\n",
    "\n",
    "# Plot the singular values\n",
    "plt.figure(figsize=(10, 2.5))\n",
    "plt.plot(S.cpu(), marker='o', linestyle='-', color='b')\n",
    "plt.title(f'Singular Values of Llama layer {layer_name}')\n",
    "plt.xlabel('Index', color=\"white\")\n",
    "plt.ylabel('Singular Value', color=\"white\")\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.title(f\"Log Magnitude of Llama Weights ({layer_name})\")\n",
    "plt.imshow(torch.log10(torch.abs(X_lla)).cpu(), interpolation='nearest')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 5))\n",
    "ax1, ax2, ax3 = fig.subplots(1, 3)\n",
    "fig.suptitle(f\"Llama: {layer_name}\")\n",
    "\n",
    "ax1.set_title(\"2D DCT (dB)\")\n",
    "pc = ax1.imshow(20*torch.log10(torch.abs(dct.dct_2d(X_lla, norm='ortho'))).cpu(), interpolation='nearest')\n",
    "fig.colorbar(pc, location=\"bottom\")\n",
    "\n",
    "ax2.set_title(f\"Row DCT (dB)\")\n",
    "pc = ax2.imshow(20*torch.log10(torch.abs(dct.dct(X_lla.T, norm='ortho').T)).cpu(), interpolation='nearest')\n",
    "fig.colorbar(pc, location=\"bottom\")\n",
    "\n",
    "ax3.set_title(f\"Col DCT (dB)\")\n",
    "pc = ax3.imshow(20*torch.log10(torch.abs(dct.dct(X_lla, norm='ortho'))).cpu(), interpolation='nearest')\n",
    "fig.colorbar(pc, location=\"bottom\")\n",
    "plt.show()\n",
    "# plt.title(\"2D DCT (dB) of Mistral Weight\")\n",
    "# plt.imshow(20*torch.log10(torch.abs(dct.dct_2d(X_mis, norm='ortho'))).cpu(), interpolation='nearest')\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_lla\n",
    "\n",
    "wavelet = 'haar'\n",
    "CA, (CH, CV, CD) = pywt.dwt2(X.cpu(), wavelet)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "ax1, ax2, ax3, ax4 = fig.subplots(1, 4)\n",
    "\n",
    "fig.suptitle(f\"{wavelet} Wavelet Coefficients Magnitudes (dB): Mistral Weight\")\n",
    "\n",
    "pc = ax1.imshow(20*np.log10(np.abs(CA) + np.max(np.abs(CA))*1e-6), aspect='auto', interpolation='nearest')\n",
    "fig.colorbar(pc, location=\"bottom\")\n",
    "\n",
    "pc = ax2.imshow(20*np.log10(np.abs(CH) + np.max(np.abs(CH))*1e-6), aspect='auto', interpolation='nearest')\n",
    "fig.colorbar(pc, location=\"bottom\")\n",
    "\n",
    "pc = ax3.imshow(20*np.log10(np.abs(CV) + np.max(np.abs(CV))*1e-6), aspect='auto', interpolation='nearest')\n",
    "fig.colorbar(pc, location=\"bottom\")\n",
    "\n",
    "pc = ax4.imshow(20*np.log10(np.abs(CD) + np.max(np.abs(CD))*1e-6), aspect='auto', interpolation='nearest')\n",
    "fig.colorbar(pc, location=\"bottom\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_lla\n",
    "\n",
    "level = 1e-2\n",
    "\n",
    "for family in pywt.families()[:7]:\n",
    "    for wav in pywt.wavelist(family)[:5]:\n",
    "        \n",
    "        CA, (CH, CV, CD) = pywt.dwt2(X.cpu(), wav)\n",
    "        sparsity = sum([np.sum(np.abs(C) > np.max(np.abs(C)) * 1e-2) / \\\n",
    "                        (C.shape[1]*C.shape[0]) for C in [CA, CH, CV, CD]]) / 4\n",
    "        print(f\"{wav}: proportion of elements larger than {level} * maximum value: {sparsity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_dct_error(X, ratio_retained=0.5):\n",
    "    \"\"\"\n",
    "    Takes the DCT of X and zeros out the high-frequency components,\n",
    "    and then computes the Frobenius norm error incurred by only\n",
    "    retaining the low-frequency components\n",
    "    \"\"\"\n",
    "    X_DCT = dct.dct_2d(X)\n",
    "    n, d = X.shape\n",
    "    num_rows = int(n*np.sqrt(ratio_retained))\n",
    "    num_cols = int(d*np.sqrt(ratio_retained))\n",
    "    X_DCT[num_rows:, :] = 0\n",
    "    X_DCT[:, num_cols:] = 0\n",
    "    X_hat = dct.idct_2d(X_DCT)\n",
    "    return torch.norm(X - X_hat, p=\"fro\").item() / torch.norm(X, p=\"fro\").item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_dct_error(X, ratio_retained=0.5):\n",
    "    \"\"\"\n",
    "    Takes the DCT of X and zeros out the components with the lowest magnitude,\n",
    "    and then computes the Frobenius norm error incurred.\n",
    "    \"\"\"\n",
    "    X_DCT = dct.dct_2d(X)\n",
    "    n, d = X.shape\n",
    "    X_DCT = make_sparse(X_DCT, int(n*d*ratio_retained))\n",
    "    X_hat = dct.idct_2d(X_DCT)\n",
    "    return torch.norm(X - X_hat, p=\"fro\").item() / torch.norm(X, p=\"fro\").item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio=0.5\n",
    "print(f\"Llama: Relative error for iDCT(DCT(X)): {top_k_dct_error(X_lla, ratio_retained=1)}\")\n",
    "print(f\"Llama: Relative error for top-k DCT({int(ratio*100)}% retained): {top_k_dct_error(X_lla, ratio_retained=ratio)}\")\n",
    "print(f\"Llama: Relative error for high-freq truncated DCT({int(ratio*100)}% retained): {truncated_dct_error(X_lla, ratio_retained=ratio)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_shepp_logan = torch.tensor(shepp_logan(2048)).to(DEFAULT_DEVICE)\n",
    "print(f\"Shepp-Logan: Relative error for iDCT(DCT(X)): {top_k_dct_error(X_shepp_logan, ratio_retained=1)}\")\n",
    "print(f\"Shepp-Logan: Relative error for top-k DCT({int(ratio*100)}% retained): {top_k_dct_error(X_shepp_logan, ratio_retained=ratio)}\")\n",
    "print(f\"Shepp-Logan: Relative error for high-freq truncated DCT({int(ratio*100)}% retained): {truncated_dct_error(X_shepp_logan, ratio_retained=ratio)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations Between Different Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name1, X_lla1 = list(llama.named_parameters())[109]\n",
    "layer_name2, X_lla2 = list(llama.named_parameters())[110]\n",
    "X_lla1 = X_lla1.detach()\n",
    "X_lla2 = X_lla2.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot two layers side-by-side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_idxs = (0, 4096)\n",
    "y_idxs = (0, 4096)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 4))\n",
    "ax1, ax2 = fig.subplots(1, 2)\n",
    "\n",
    "ax1.set_title(f\"Log Magnitude of Llama Weights ({layer_name1})\")\n",
    "pc = ax1.imshow(torch.log10(torch.abs(X_lla1[y_idxs[0]:y_idxs[1], x_idxs[0]:x_idxs[1]])).cpu(), interpolation='nearest')\n",
    "fig.colorbar(pc)\n",
    "\n",
    "ax2.set_title(f\"Log Magnitude of Llama Weights ({layer_name2})\")\n",
    "pc = ax2.imshow(torch.log10(torch.abs(X_lla2[y_idxs[0]:y_idxs[1], x_idxs[0]:x_idxs[1]])).cpu(), interpolation='nearest')\n",
    "fig.colorbar(pc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized inner product between the two layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.trace(X_lla1.T @ X_lla2) / (torch.norm(X_lla2, p=\"fro\") * torch.norm(X_lla1, p=\"fro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 4))\n",
    "ax1, ax2 = fig.subplots(1, 2)\n",
    "inner_prods = (X_lla1 @ X_lla2.T) / (torch.norm(X_lla1, dim=1)*torch.norm(X_lla2, dim=1))\n",
    "inner_prods = inner_prods#[:100, :100]\n",
    "pc = ax1.imshow(torch.log10(torch.abs(inner_prods)).cpu(), interpolation='nearest')\n",
    "fig.colorbar(pc)\n",
    "ax1.set_title(\"Row correlations\")\n",
    "\n",
    "ax2.set_title(\"Row correlations > 0.02\")\n",
    "pc = ax2.imshow(torch.abs(inner_prods) > 0.02, interpolation=\"nearest\")\n",
    "fig.colorbar(pc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "Experiments with different versions of `IterativeWeightDecomposition`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint optimization between two layers\n",
    "\n",
    "\"Shared Q\": given two weight matrices, compute\n",
    "\n",
    "$$X_1 \\approx Q + S_1 + L_1 R_1$$\n",
    "$$X_2 \\approx Q + S_2 + L_2 R_2$$\n",
    "via alternating minimization, where both layers have the same $Q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a key matrix and query matrix\n",
    "layer_name1, X_lla1 = list(llama.named_parameters())[37]\n",
    "layer_name2, X_lla2 = list(llama.named_parameters())[38]\n",
    "X_lla1 = X_lla1.detach().to(DEFAULT_DEVICE)\n",
    "X_lla2 = X_lla2.detach().to(DEFAULT_DEVICE)\n",
    "(layer_name1, layer_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_lla1.shape == X_lla2.shape\n",
    "n, d = X_lla1.shape\n",
    "(n, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the bit budgets for LoftQ? We want to get similar performance at a lower bit budget (or, maybe, better performance at the same bit budget)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LoftQ with 4-bit quant: %e bits per layer\" % (4*n*d + 16*64*(n+d)))\n",
    "print(\"LoftQ with 2-bit quant: %e bits per layer\" % (2*n*d + 16*64*(n+d)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formulation 1: Q is a full matrix, and we learn separate low-rank and sparse components for both weight matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: because of the space it takes to store the quantized bitmask, we can't\n",
    "# achieve the bitrate of 2-bit LoftQ while including a sparse component.\n",
    "# Instead, we increase k\n",
    "layers = [X_lla1, X_lla2]\n",
    "n_layers = len(layers)\n",
    "\n",
    "# k = rank of L, R\n",
    "# BQ, BS, BLR = quantization levels of the respective matrices\n",
    "k=64; BQ = 2; BS = 4; BLR = 16\n",
    "# nonzero_ratio: fraction of S that will be nonzero. Make this 0\n",
    "# to omit S entirely.\n",
    "nonzero_ratio = 0.25\n",
    "\n",
    "# LR bits \n",
    "total_bits = k*(n+d)*BLR*n_layers + BQ*n*d + \\\n",
    "    BS*nonzero_ratio*n*d*n_layers + n_layers*(nonzero_ratio > 0)*n*d\n",
    "\n",
    "print(\"Bits per layer: %e\" % (total_bits / n_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decomp_shared_Q = SharedQWeightDecomposition(\n",
    "    Xs=layers,\n",
    "    params=IterativeWeightDecompositionParams(\n",
    "        compute_quantized_component=True,\n",
    "        compute_low_rank_factors=True,\n",
    "        compute_sparse_component=nonzero_ratio > 0,\n",
    "        lplr_params=LPLRParameters(\n",
    "            bits=BLR,\n",
    "            rank=k\n",
    "        ),\n",
    "        bits_quant=BQ,\n",
    "        bits_sparse=BS,\n",
    "        sparse_ratio_nonzeros=nonzero_ratio,\n",
    "        quantizer_factory=NFQuantizerFactory(\n",
    "            method=\"normal\", low_memory_quantizer=True\n",
    "        ),\n",
    "        iters=100,\n",
    "        log_errors=True,\n",
    "        error_norm=DEFAULT_ERROR_NORM\n",
    "    )\n",
    ")\n",
    "weight_decomp_shared_Q.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(weight_decomp_shared_Q.avg_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decomp_shared_Q.plot_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=64; BQ = 4; BS = 4; BLR = 16; shared_ratio = 0.5\n",
    "bits = k*(n+d)*BLR*2 + BQ*n*d*shared_ratio + BS*(1-shared_ratio)*2*n*d + min(shared_ratio, 1-shared_ratio)*n\n",
    "print(\"Bits per layer: %e\" % (bits / 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decomp_shared_Q_corrs = SharedQWeightDecompositionWithCorrelations(\n",
    "    Xs=[X_lla1, X_lla2],\n",
    "    shared_ratio=shared_ratio,\n",
    "    params=IterativeWeightDecompositionParams(\n",
    "        compute_low_rank_factors=True,\n",
    "        lplr_params=LPLRParameters(\n",
    "            bits=BLR,\n",
    "            rank=k\n",
    "        ),\n",
    "        bits_quant=BQ,\n",
    "        bits_sparse=BS,\n",
    "        quantizer_factory=NFQuantizerFactory(\n",
    "            method=\"normal\", low_memory_quantizer=True\n",
    "        ),\n",
    "        iters=100,\n",
    "        log_errors=True,\n",
    "        error_norm=DEFAULT_ERROR_NORM\n",
    "    )\n",
    ")\n",
    "weight_decomp_shared_Q_corrs.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(weight_decomp_shared_Q_corrs.errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decomp_shared_Q_corrs.plot_errors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison: LoftQ-LPLR with 2-bit uniform quantization (NF2 quantization leads to very high spectral error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decomp_loftq = IterativeWeightDecomposition(\n",
    "    X=X_lla1,\n",
    "    params=IterativeWeightDecompositionParams(\n",
    "        compute_quantized_component=True,\n",
    "        compute_low_rank_factors=True,\n",
    "        lplr_params=LPLRParameters(\n",
    "            rank=64,\n",
    "            bits=16\n",
    "        ),\n",
    "        quantizer_factory=NFQuantizerFactory(\n",
    "            method=\"uniform\", low_memory_quantizer=True\n",
    "        ),\n",
    "        bits_quant=2,\n",
    "        iters=50,\n",
    "        log_errors=True,\n",
    "        error_norm=DEFAULT_ERROR_NORM\n",
    "    )\n",
    ")\n",
    "weight_decomp_loftq.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(weight_decomp_loftq.errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decomp_loftq.plot_errors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoftQ-LPLR with 4-bit NF quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decomp_loftq_4b = IterativeWeightDecomposition(\n",
    "    X=X_lla1,\n",
    "    params=IterativeWeightDecompositionParams(\n",
    "        compute_quantized_component=True,\n",
    "        compute_low_rank_factors=True,\n",
    "        lplr_params=LPLRParameters(\n",
    "            rank=64,\n",
    "            bits=16\n",
    "        ),\n",
    "        quantizer_factory=NFQuantizerFactory(\n",
    "            method=\"normal\", low_memory_quantizer=True\n",
    "        ),\n",
    "        bits_quant=4,\n",
    "        iters=50,\n",
    "        log_errors=True,\n",
    "        error_norm=DEFAULT_ERROR_NORM\n",
    "    )\n",
    ")\n",
    "weight_decomp_loftq_4b.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decomp_loftq_4b.plot_errors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADMM and Variations\n",
    "Inspired by [Alternating Direction Method of Multipliers for Quantization](https://arxiv.org/pdf/2009.03482.pdf) (Huang et al, 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name, X_lla = list(llama.named_parameters())[48]\n",
    "X_lla = X_lla.detach().to(DEFAULT_DEVICE)\n",
    "n, d = X_lla.shape\n",
    "(n, d, layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basis of comparison: not using ADMM\n",
    "weight_decomp = IterativeWeightDecomposition(\n",
    "    X=X_lla,\n",
    "    label=\"LoftQ-LPLR\",\n",
    "    params=IterativeWeightDecompositionParams(\n",
    "        compute_quantized_component=True,\n",
    "        compute_low_rank_factors=True,\n",
    "        compute_sparse_component=False,\n",
    "        lplr_params=LPLRParameters(\n",
    "            bits=[8],\n",
    "            rank=[64]\n",
    "        ),\n",
    "        bits_quant=4,\n",
    "        quantizer_factory=NFQuantizerFactory(\n",
    "            method=\"normal\", low_memory_quantizer=True\n",
    "        ),\n",
    "        iters=150,\n",
    "        log_errors=True,\n",
    "        error_norm=DEFAULT_ERROR_NORM\n",
    "    )\n",
    ")\n",
    "weight_decomp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(weight_decomp.errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decomp.plot_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decomp_admm = ADMMWeightDecomposition(\n",
    "    X=X_lla,\n",
    "    label=\"ADMM-Q\",\n",
    "    params=IterativeWeightDecompositionParams(\n",
    "        compute_quantized_component=True,\n",
    "        compute_low_rank_factors=True,\n",
    "        compute_sparse_component=False,\n",
    "        lplr_params=LPLRParameters(\n",
    "            bits=[8],\n",
    "            rank=[64]\n",
    "        ),\n",
    "        bits_quant=4,\n",
    "        quantizer_factory=NFQuantizerFactory(\n",
    "            method=\"normal\", low_memory_quantizer=True\n",
    "        ),\n",
    "        iters=150,\n",
    "        log_errors=True,\n",
    "        error_norm=DEFAULT_ERROR_NORM\n",
    "    )\n",
    ")\n",
    "weight_decomp_admm.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(weight_decomp_admm.errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decomp_admm.plot_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decomp_admm.plot_errors(plot_errors=False, plot_lagrangians=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decomp_admm2 = ADMMWeightDecomposition(\n",
    "    X=X_lla,\n",
    "    label=\"ADMM-R\",\n",
    "    params=IterativeWeightDecompositionParams(\n",
    "        compute_quantized_component=True,\n",
    "        compute_low_rank_factors=True,\n",
    "        compute_sparse_component=False,\n",
    "        lplr_params=LPLRParameters(\n",
    "            bits=[8],\n",
    "            rank=[64]\n",
    "        ),\n",
    "        bits_quant=4,\n",
    "        quantizer_factory=NFQuantizerFactory(\n",
    "            method=\"normal\", low_memory_quantizer=True\n",
    "        ),\n",
    "        iters=150,\n",
    "        log_errors=True,\n",
    "        error_norm=DEFAULT_ERROR_NORM,\n",
    "    ),\n",
    "    admm_params=ADMMParameters(\n",
    "        admm_type=ADMMType.ADMM_R,\n",
    "        admm_r_update_p=0.75\n",
    "    )\n",
    ")\n",
    "weight_decomp_admm2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(weight_decomp_admm2.errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decomp_admm2.plot_errors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADMM seems to be about identical to LoftQ-LPLR, including the randomized and soft-quantized variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a sparse component, maybe\n",
    "\n",
    "And mixed-precision quantization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name, X_lla = list(llama.named_parameters())[46]\n",
    "X_lla = X_lla.detach().to(DEFAULT_DEVICE)\n",
    "n, d = X_lla.shape\n",
    "(n, d, layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loftq_rank = 64\n",
    "loftq_bits = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LoftQ with 2-bit quant: %e bits\" % (2*n*d + 16*loftq_rank*(n+d)))\n",
    "print(\"LoftQ with 4-bit quant: %e bits\" % (4*n*d + 16*loftq_rank*(n+d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decomp_loftq = IterativeWeightDecomposition(\n",
    "    X=X_lla,\n",
    "    label=\"LoftQ\",\n",
    "    params=IterativeWeightDecompositionParams(\n",
    "        compute_quantized_component=True,\n",
    "        compute_low_rank_factors=True,\n",
    "        compute_sparse_component=False,\n",
    "        lplr_params=LPLRParameters(\n",
    "            bits=[16],\n",
    "            rank=[loftq_rank]\n",
    "        ),\n",
    "        bits_quant=loftq_bits,\n",
    "        quantizer_factory=NFQuantizerFactory(\n",
    "            method=\"uniform\", low_memory_quantizer=True\n",
    "        ),\n",
    "        iters=100,\n",
    "        log_errors=True,\n",
    "        error_norm=DEFAULT_ERROR_NORM\n",
    "    )\n",
    ")\n",
    "weight_decomp_loftq.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(weight_decomp_loftq.errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decomp_loftq.plot_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = np.array([64])\n",
    "factor_bits = np.array([16])\n",
    "\n",
    "nonzero_ratio=0.25\n",
    "BQ=0\n",
    "BS=4\n",
    "\n",
    "total_bits = BQ*n*d + \\\n",
    "    np.sum(ranks * factor_bits)*(n+d) + \\\n",
    "        (nonzero_ratio > 0)*n*d + \\\n",
    "        n*d*nonzero_ratio*BS\n",
    "print(\"Bit budget: %e\" % total_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decomp_w_sparse = IterativeWeightDecomposition(\n",
    "    X=X_lla,\n",
    "    label=\"LoftQ-LPLR + Sparse\",\n",
    "    params=IterativeWeightDecompositionParams(\n",
    "        compute_quantized_component=False,\n",
    "        compute_low_rank_factors=True,\n",
    "        compute_sparse_component=True,\n",
    "        lplr_params=LPLRParameters(\n",
    "            bits=factor_bits,\n",
    "            rank=ranks\n",
    "        ),\n",
    "        bits_quant=BQ,\n",
    "        bits_sparse=BS,\n",
    "        sparse_ratio_nonzeros=nonzero_ratio,\n",
    "        quantizer_factory=NFQuantizerFactory(\n",
    "            method=\"uniform\", low_memory_quantizer=True\n",
    "        ),\n",
    "        iters=100,\n",
    "        log_errors=True,\n",
    "        error_norm=DEFAULT_ERROR_NORM\n",
    "    )\n",
    ")\n",
    "weight_decomp_w_sparse.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(weight_decomp_w_sparse.errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decomp_w_sparse.plot_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decomp_no_Q = IterativeWeightDecomposition(\n",
    "    X=X_lla,\n",
    "    label=\"LoftQ-LPLR + Sparse\",\n",
    "    params=IterativeWeightDecompositionParams(\n",
    "        compute_quantized_component=False,\n",
    "        compute_low_rank_factors=True,\n",
    "        compute_sparse_component=False,\n",
    "        lplr_params=LPLRParameters(\n",
    "            bits=[16, 4],\n",
    "            rank=[64, 1024]\n",
    "        ),\n",
    "        quantizer_factory=NFQuantizerFactory(\n",
    "            method=\"uniform\", low_memory_quantizer=True\n",
    "        ),\n",
    "        iters=100,\n",
    "        log_errors=True,\n",
    "        error_norm=DEFAULT_ERROR_NORM\n",
    "    )\n",
    ")\n",
    "weight_decomp_no_Q.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(weight_decomp_no_Q.errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decomp_no_Q.plot_errors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Pruning (inspired by Aaron's weight-pruning technique)\n",
    "\n",
    "This method is like LoftQ-LPLR, but it replaces the $Q$ matrix with the following:\n",
    "\n",
    "The [Michael Mahoney CUR Paper](https://www.pnas.org/doi/10.1073/pnas.0803205106) chooses columns to select based on their correlation with the $k$ top left signular vectors, computed via the following levarage scores:\n",
    "\n",
    "$$\\pi_j = \\frac{1}{k} \\sum_{\\xi = 1}^k \\left(\\mathbf{v}_j^\\xi\\right)^2.$$\n",
    "\n",
    "We choose to keep the columns with the $k$ highest leverage scores and try to represent the rest as a linear combination of these columns.\n",
    "The matrix of retained columns is denoted $Q_\\text{left}$, and the pruned columns as $Q_{-\\text{cols}}$.\n",
    "\n",
    "We use least squares to find $Q_\\text{right}$ such that $Q_\\text{left}Q_\\text{right} \\approx Q_{-\\text{cols}}$ and $Q_\\text{left} \\begin{bmatrix} I_k Q_\\text{right} \\end{bmatrix} P \\approx Q$, where $P$ is a column permutation matrix.\n",
    "\n",
    "Then, we quantize $Q_\\text{left}$ and use alternating least squares to find quantized components such that $Q_\\text{left} \\begin{bmatrix} I_k Q_\\text{right} \\end{bmatrix} P \\approx Q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name, X_lla = list(llama.named_parameters())[49]\n",
    "X_lla = X_lla.detach().to(DEFAULT_DEVICE)\n",
    "n, d = X_lla.shape\n",
    "(n, d, layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2B Quant LoftQ: %e bits\" % (n * d * 2 + 16*64*(n+d)))\n",
    "print(\"4B Quant LoftQ: %e bits\" % (n * d * 4 + 16*64*(n+d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_cols = 2944\n",
    "rank = 64\n",
    "BLR = 16\n",
    "BQ = 4\n",
    "\n",
    "print(\"%e bits\" % (n*(d - pruned_cols)*BQ + pruned_cols*(d-pruned_cols)*BQ  + d + BLR*rank*(n+d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decomp_pruned = ColumnPrunedDecomposition(\n",
    "    X=X_lla,\n",
    "    params=IterativeWeightDecompositionParams(\n",
    "        lplr_params=LPLRParameters(\n",
    "            bits=BLR,\n",
    "            rank=rank\n",
    "        ),\n",
    "        bits_quant=BQ,\n",
    "        quantizer_factory=NFQuantizerFactory(\n",
    "            method=\"uniform\", low_memory_quantizer=True\n",
    "        ),\n",
    "        iters=20,\n",
    "        log_errors=True,\n",
    "        error_norm=DEFAULT_ERROR_NORM,\n",
    "        rand_svd=True\n",
    "    ),\n",
    "    pruned_cols=pruned_cols,\n",
    ")\n",
    "weight_decomp_pruned.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(weight_decomp_pruned.errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decomp_pruned.plot_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
