{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LPLR-Q: Adding an Activation Between L and R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glog\n",
    "glog.setLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lplr_llm.activation_aware.weight_compression import *\n",
    "from lplr_llm.activation_aware.layer_quantization import *\n",
    "from lplr_llm.activation_aware.layer_input_data import get_sublayer_input\n",
    "from torch import nn\n",
    "import torch\n",
    "from lib.utils import get_hadK\n",
    "from lib.utils.matmul_had import matmul_hadU_cuda, matmul_hadUt_cuda\n",
    "from tqdm import tqdm\n",
    "from lib.algo.quip import RHT_W, RHT_H\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"meta-llama/Llama-2-7b-hf\"\n",
    "# HESSIAN_SAVE_PATH = \"/media/hdd1/lplr-q-hessians/llama-2-7b\"\n",
    "HESSIAN_SAVE_PATH = \"../../data/hessians/llama-2-7b/\"\n",
    "DEVICE = \"cuda:0\"\n",
    "RANK = 128\n",
    "QLR_ITERS = 30\n",
    "LPLR_ITERS = 10\n",
    "\n",
    "LAYER = 5\n",
    "SUBLAYER = TransformerSubLayers.QUERY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Initial Condition for Q, L, and R using LPLR-Q\n",
    "\n",
    "The three weight compressor options below compute $Q + LR$, where $Q$ has 2 bits of precision and $L$, $R$ are half precision. Below, there are three options for quantization of $Q$ (LDLQ, lattice quantization, and RTN uniform quantization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 1**: LPLR-LDLQ with Hessian Downdate (should be a very good approximation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_comp = ActivationAwareWeightCompressor(\n",
    "#     model_params=ModelParameters(\n",
    "#         base_model=BASE_MODEL\n",
    "#     ),\n",
    "#     data_params=DataParameters(),\n",
    "#     hessian_save_path=HESSIAN_SAVE_PATH,\n",
    "#     quant_params=ActivationAwareQuantParams(\n",
    "#         Q_bits=2,\n",
    "#         L_bits=16, R_bits=16,\n",
    "#         lattice_quant_LR=False,\n",
    "#         rank=RANK,\n",
    "#         activation_aware_Q=True,\n",
    "#         activation_aware_LR=True,\n",
    "#         hadamard_transform=True,\n",
    "#         iters=3,\n",
    "#         lplr_iters=LPLR_ITERS,\n",
    "#         rand_svd=True,\n",
    "#         Q_hessian_downdate=True,\n",
    "#         update_order=[\"Q\", \"LR\"]\n",
    "#     ),\n",
    "#     compute_hessians=False,\n",
    "#     quant_device=DEVICE,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2**: LPLR-Lattice Quant (slightly worse approximatiom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_comp = ActivationAwareWeightCompressor(\n",
    "#     model_params=ModelParameters(\n",
    "#         base_model=BASE_MODEL\n",
    "#     ),\n",
    "#     data_params=DataParameters(),\n",
    "#     hessian_save_path=HESSIAN_SAVE_PATH,\n",
    "#     quant_params=ActivationAwareQuantParams(\n",
    "#         Q_bits=2,\n",
    "#         L_bits=16, R_bits=16,\n",
    "#         lattice_quant_LR=False,\n",
    "#         rank=RANK,\n",
    "#         activation_aware_Q=False,\n",
    "#         activation_aware_LR=True,\n",
    "#         hadamard_transform=True,\n",
    "#         iters=QLR_ITERS,\n",
    "#         lplr_iters=LPLR_ITERS,\n",
    "#         rand_svd=True,\n",
    "#         update_order=[\"Q\", \"LR\"]\n",
    "#     ),\n",
    "#     compute_hessians=False,\n",
    "#     quant_device=DEVICE,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 3**: LPLR-Uniform ($Q$ should be a pretty bad approximation of $W - LR$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_comp = ActivationAwareWeightCompressor(\n",
    "    model_params=ModelParameters(\n",
    "        base_model=BASE_MODEL\n",
    "    ),\n",
    "    data_params=DataParameters(),\n",
    "    hessian_save_path=HESSIAN_SAVE_PATH,\n",
    "    quant_params=ActivationAwareQuantParams(\n",
    "        Q_bits=2,\n",
    "        L_bits=16, R_bits=16,\n",
    "        lattice_quant_LR=False,\n",
    "        rank=RANK,\n",
    "        activation_aware_Q=False,\n",
    "        lattice_quant_Q=False,\n",
    "        quant_factory_Q=QuantizerFactory(\"uniform\"),\n",
    "        activation_aware_LR=True,\n",
    "        hadamard_transform=True,\n",
    "        iters=QLR_ITERS,\n",
    "        lplr_iters=LPLR_ITERS,\n",
    "        rand_svd=True,\n",
    "        Q_hessian_downdate=True,\n",
    "        update_order=[\"Q\", \"LR\"]\n",
    "    ),\n",
    "    compute_hessians=False,\n",
    "    quant_device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_quant = weight_comp.get_layer_quantizer(LAYER)\n",
    "layer_quant.compress_sublayer(SUBLAYER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The Frobenius norm error ||(W - LR - Q)X||_F^2 is {round(layer_quant.min_error(SUBLAYER) * 100, 8)} percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Calibration Data and Compute the Layer Input\n",
    "\n",
    "See `src/lplr_llm/activation_aware/layer_input_data` for more details. It is very similar to the code that QuIP# uses for Hessian computation.\n",
    "The function `get_sublayer_input` passes the calibration datapoints through the original model and captures the input to the specific layer and sublayer that we are trying to compress. \n",
    "\n",
    "_Note_: this might not be the data used to compute the Hessian matrix (as the Hessian computation does not currently include saving the input embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, sublayer_input, attention_mask, position_ids = \\\n",
    "    get_sublayer_input(\n",
    "        LAYER, layer_quant.sublayer_info[SUBLAYER].out_key,\n",
    "        BASE_MODEL, data_params=DataParameters(batch_size=BATCH_SIZE),\n",
    "        device=DEVICE\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Approximation as a Neural Network\n",
    "\n",
    "This neural network layer does the following:\n",
    "1. Performs an inverse Hadamard transform on the input, $X$. The Hadamard-transformed output will be denoted $X_H$. This is because the we perform the decomposition $Q + LR$ after incoherence processing: $H_2 W H_1^\\top \\approx Q + LR$, where $H_1$ and $H_2$ are randomized Hadamard transform matrices, $Q$ is quantized, and $L, R$ are low-rank.\n",
    "\n",
    "2. Applies $Q$: $\\text{output} \\gets X_H Q^\\top$.\n",
    "3. Applies the low-rank factors as follows:\n",
    "    - If `network_type` is `REGULAR_MLP`, $\\text{output} \\gets \\text{output} + \\sigma(X_H R^\\top) L^\\top$, where $\\sigma$ is an activation, by default ReLU.\n",
    "    - If `network_type` is `SPLIT_LR`, we split $L = \\begin{pmatrix} \\\\ L_1 & L_2 \\\\ \\\\ \\end{pmatrix}$ and $R = \\begin{pmatrix} && R_1 && \\\\ && R_2 && \\end{pmatrix}$, where $L_1$ and $L_2$ are of equal size and the same holds for $R_1$ and $R_2$.\n",
    "    <br/> The output is updated as follows:  $\\text{output} \\gets \\text{output} + \\sigma(X_H R_1^\\top) L_1^\\top + X_H R_2^\\top L_2^\\top$.\n",
    "    - If `network_type` is `WITH_RESIDUAL`, we add skip connections without splitting the $L$ and $R$ matrices: $\\text{output} \\gets \\text{output} + \\frac{1}{2}\\sigma(X_H R^\\top) L^\\top + \\frac{1}{2}X_H R^\\top L^\\top$\n",
    "    - If you pass in `batchnorm=True` when instantiating the network, then a batch norm (via the default pytorch implementation) is applied after the activation: $\\sigma(X_H R^\\top) L^\\top$ becomes $\\text{BN}(\\sigma(X_H R^\\top)) L^\\top$, where $\\text{BN}$ is the batch norm function.\n",
    "\n",
    "4. Perform a Hadamard transform on $\\text{output}$ and returns the final result.\n",
    "\n",
    "#### Explanation of the incoherence processing (steps 1 and 4 above)\n",
    "For the purposes of illustration, assume that $H_1^\\top W H_2 = Q + LR$ (the decomposition is exact), and that the network just computes $X_H (Q + LR)^\\top$.\n",
    "\n",
    "The network first computes $X_H = X H_1^\\top$ in step 1.\n",
    "\n",
    "Then, it computes $\\text{output} = X_H W_H^\\top = X H_1^\\top H_1 W^\\top H_2^\\top = X W^\\top H_2^\\top$.\n",
    "\n",
    "Step 4 computes $\\text{output} H_2 = X W^\\top H_2^\\top H_2 = XW^\\top$, which is the desired neutral network output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRNetworkType(Enum):\n",
    "    REGULAR_MLP = 0\n",
    "    SPLIT_LR = 1\n",
    "    WITH_RESIDUAL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QPlusTwoLayerNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        Q, L, R,\n",
    "        SU, SV,\n",
    "        global_scale,\n",
    "        scaleWH,\n",
    "        act=nn.ReLU(),\n",
    "        batchnorm=True,\n",
    "        network_type=LRNetworkType.REGULAR_MLP\n",
    "    ):\n",
    "        super(QPlusTwoLayerNN, self).__init__()\n",
    "\n",
    "        # Scaling parameters -- from QuIP#\n",
    "        self.global_scale = global_scale\n",
    "        self.scaleWH = scaleWH\n",
    "        if self.scaleWH is not None:\n",
    "            self.scaleWH = nn.Parameter(self.scaleWH.float(), requires_grad=False)\n",
    "        \n",
    "        # Quantized matrix. For the purpose of this experiment, Q is floating point, i.e.,\n",
    "        # only simulated quantization was performed.\n",
    "        self.Q = nn.Parameter(Q.float(), requires_grad=False)\n",
    "\n",
    "        if network_type is LRNetworkType.WITH_RESIDUAL:\n",
    "            L = L / 2\n",
    "        if network_type is LRNetworkType.SPLIT_LR:\n",
    "            # Split L and R, as described in the markdown above\n",
    "            self.L = nn.Parameter(L.float()[:, :L.shape[1]//2], requires_grad=True)\n",
    "            self.R = nn.Parameter(R.float()[:R.shape[0]//2, :], requires_grad=True)\n",
    "\n",
    "            self.L_feedthrough = nn.Parameter(L.float()[:, L.shape[1]//2:], requires_grad=True)\n",
    "            self.R_feedthrough = nn.Parameter(R.float()[R.shape[0]//2:, :], requires_grad=True)\n",
    "            self.norm = nn.BatchNorm1d(L.shape[1] // 2, affine=False).to(self.L.device)\n",
    "        else:\n",
    "            self.L = nn.Parameter(L.float(), requires_grad=True)\n",
    "            self.R = nn.Parameter(R.float(), requires_grad=True)\n",
    "            self.norm = nn.BatchNorm1d(L.shape[1], affine=False).to(self.L.device)\n",
    "        \n",
    "        self.network_type = network_type\n",
    "        self.do_batchnorm = batchnorm\n",
    "\n",
    "        self.act = act\n",
    "\n",
    "        # Diagonals of the randomized Hadamard transform\n",
    "        self.SU = nn.Parameter(SU.float(), requires_grad=False)\n",
    "        self.SV = nn.Parameter(SV.float(), requires_grad=False)\n",
    "\n",
    "        # Hadamard matrices and sizes of the Hadamard transform\n",
    "        had_left, K_left = get_hadK(len(SU))\n",
    "        had_right, K_right = get_hadK(len(SV))\n",
    "        self.had_left = nn.Parameter(had_left, requires_grad=False)\n",
    "        self.had_right = nn.Parameter(had_right, requires_grad=False)\n",
    "        self.K_left = K_left\n",
    "        self.K_right = K_right\n",
    "\n",
    "    def set_activation(self, act):\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = x.shape\n",
    "        x = x.view(-1, len(self.SU))\n",
    "        \n",
    "        # Randomized Hadamard transform (see description in markdown text above)\n",
    "        x = x * self.SU \n",
    "        if self.scaleWH is not None:\n",
    "            x /= self.scaleWH\n",
    "        x = matmul_hadUt_cuda(x, self.had_left, self.K_left)\n",
    "\n",
    "        # Apply Q\n",
    "        output = x @ self.Q.T\n",
    "\n",
    "        # Different options for applying L and R, as described in the markdown above\n",
    "        if self.do_batchnorm:\n",
    "            output += self.norm(self.act(x @ self.R.T)) @ self.L.T\n",
    "        else:\n",
    "            output += self.act(x @ self.R.T) @ self.L.T\n",
    "        if self.network_type is LRNetworkType.SPLIT_LR:\n",
    "            output += x @ self.R_feedthrough.T @ self.L_feedthrough.T\n",
    "        elif self.network_type is LRNetworkType.WITH_RESIDUAL:\n",
    "            output += x @ self.R.T @ self.L.T # skip over the activation\n",
    "\n",
    "        # Another randomized Hadamard transform \n",
    "        output = matmul_hadU_cuda(output, self.had_right, self.K_right) * self.global_scale\n",
    "        output = output * self.SV\n",
    "        if self.scaleWH is not None:\n",
    "            output *= self.scaleWH\n",
    "\n",
    "        return output.view(*shape[:-1], len(self.SV))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeFroLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RelativeFroLoss, self).__init__()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        # matrix_norm defaults to Frobenius\n",
    "        return (torch.linalg.matrix_norm(target - output) / \n",
    "                torch.linalg.matrix_norm(target)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Target Layer Outputs\n",
    "Here, we compute $X W^\\top$, for each $X$ in the calibration set. We will be computing the Frobenius norm error of the network output with respect to these target outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sublayer_info = layer_quant.best_sublayer_info[SUBLAYER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.zeros(sublayer_input.shape[0], sublayer_input.shape[1], len(sublayer_info.SV)).to(sublayer_input.dtype)\n",
    "for i in tqdm(range(0, sublayer_input.shape[0], BATCH_SIZE)):\n",
    "    targets[i:i+BATCH_SIZE] = (sublayer_input[i:i+BATCH_SIZE].to(DEVICE) @ sublayer_info.W.T.to(DEVICE).half()).cpu()\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.utils.data.TensorDataset(sublayer_input, targets)\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, Perform Rank-Constrained Regression on $L$, $R$ with the Sampled Data\n",
    "The data is not necessarily the data used to compute the Hessian, and we want to make sure we start with an optimal $L$, $R$ so that we have a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, compute the Hessian\n",
    "H = torch.zeros(sublayer_input.shape[2], sublayer_input.shape[2]).to(DEVICE)\n",
    "for i in tqdm(range(sublayer_input.shape[0])):\n",
    "    x = sublayer_input[i].to(DEVICE).to(torch.float64)\n",
    "    H += x.T @ x\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "H /= H.abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization and Regularization (taken from QuIP#)\n",
    "H.div_(torch.diag(H).mean())\n",
    "H = regularize_H(H, H.shape[0], layer_quant.quant_params.quip_args.sigma_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incoherence Process\n",
    "Hr = RHT_H(H, sublayer_info.SU)\n",
    "Wr = RHT_W(sublayer_info.W.to(DEVICE) / sublayer_info.global_scale, sublayer_info.SU, sublayer_info.SV)\n",
    "\n",
    "# Compute the symmetric square root: ||(W - LR - Q)X.T||_F^2 = ||(W - LR - Q) H^{1/2}||_F^2\n",
    "eigH = torch.linalg.eigh(Hr)\n",
    "H_sqrt = (eigH.eigenvectors @\n",
    "            torch.diag(torch.sqrt(eigH.eigenvalues)) @\n",
    "            eigH.eigenvectors.T)\n",
    "\n",
    "# Rank-constrained regression\n",
    "residual = Wr - sublayer_info.Q\n",
    "Y = residual @ H_sqrt @ eigH.eigenvectors\n",
    "U, Sigma, Vh = torch.linalg.svd(Y, full_matrices=False)\n",
    "\n",
    "rank_const_regression_L = U[:, :RANK]\n",
    "rank_const_regression_R = torch.diag(Sigma[:RANK]) @ \\\n",
    "    Vh[:RANK, :] @ \\\n",
    "    torch.diag(1 / eigH.eigenvalues.sqrt()) @ eigH.eigenvectors.T\n",
    "\n",
    "sublayer_info.L = rank_const_regression_L\n",
    "sublayer_info.R = rank_const_regression_R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_type = LRNetworkType.REGULAR_MLP\n",
    "batchnorm = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is necessary, otherwise rank_const_regression_L will be modified by updates to layer.L, e.g.\n",
    "sublayer_info.L = torch.Tensor(rank_const_regression_L.tolist()).to(DEVICE)\n",
    "sublayer_info.R = torch.Tensor(rank_const_regression_R.tolist()).to(DEVICE)\n",
    "\n",
    "# Instantiate layer\n",
    "layer = QPlusTwoLayerNN(\n",
    "    Q=sublayer_info.Q, L=sublayer_info.L, R=sublayer_info.R,\n",
    "    SU=sublayer_info.SU, SV=sublayer_info.SV,\n",
    "    global_scale=sublayer_info.global_scale,\n",
    "    scaleWH=sublayer_info.scaleWH,\n",
    "    network_type=network_type,\n",
    "    batchnorm=batchnorm\n",
    ")\n",
    "loss_fn = RelativeFroLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, compute the activation-aware error for the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_act = layer.act\n",
    "layer.set_activation(nn.Identity())\n",
    "layer.do_batchnorm = False\n",
    "\n",
    "total_loss = 0\n",
    "n = 0\n",
    "with torch.no_grad():\n",
    "    for x, target in tqdm(train_loader):\n",
    "        output = layer(x.to(DEVICE).float())\n",
    "        total_loss += loss_fn(output, target.to(DEVICE).float()) \n",
    "        n += 1\n",
    "layer.act = old_act\n",
    "layer.do_batchnorm = True\n",
    "\n",
    "print(f\"The error for the linear model is {(total_loss / n).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: PReLU with initial slope=1\n",
    "This should be strictly better than the linear model, but by how much?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.set_activation(nn.PReLU(init=1).to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    params=[param for _, param in layer.named_parameters() if param.requires_grad],\n",
    "    lr=1e-4,\n",
    "    amsgrad=True\n",
    ")\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRINT_FREQ=2\n",
    "\n",
    "torch.set_grad_enabled(True)\n",
    "for epoch in range(50):\n",
    "    n = 0\n",
    "    total_loss = 0\n",
    "    for x, target in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = layer(x.to(DEVICE).float())\n",
    "        loss = loss_fn(output, target.to(DEVICE).float())\n",
    "        total_loss += loss.item()\n",
    "        n += 1\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    if epoch % PRINT_FREQ == 0:\n",
    "        print(total_loss / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "total_loss = 0\n",
    "n = 0\n",
    "with torch.no_grad():\n",
    "    for x, target in tqdm(train_loader):\n",
    "        output = layer(x.to(DEVICE).float())\n",
    "        total_loss += loss_fn(output, target.to(DEVICE).float()) \n",
    "        n += 1\n",
    "(total_loss / n).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.act.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is necessary, otherwise original_L will be modified by updates to layer.L, e.g.\n",
    "sublayer_info.L = torch.Tensor(rank_const_regression_L.tolist()).to(DEVICE)\n",
    "sublayer_info.R = torch.Tensor(rank_const_regression_R.tolist()).to(DEVICE)\n",
    "\n",
    "# Instantiate layer\n",
    "layer = QPlusTwoLayerNN(\n",
    "    Q=sublayer_info.Q, L=sublayer_info.L, R=sublayer_info.R,\n",
    "    SU=sublayer_info.SU, SV=sublayer_info.SV,\n",
    "    global_scale=sublayer_info.global_scale,\n",
    "    scaleWH=sublayer_info.scaleWH\n",
    ")\n",
    "loss_fn = RelativeFroLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    params=[param for _, param in layer.named_parameters() if param.requires_grad],\n",
    "    lr=1e-4,\n",
    ")\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRINT_FREQ=2\n",
    "\n",
    "torch.set_grad_enabled(True)\n",
    "for epoch in range(20):\n",
    "    n = 0\n",
    "    total_loss = 0\n",
    "    for x, target in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = layer(x.to(DEVICE).float())\n",
    "        loss = loss_fn(output, target.to(DEVICE).float())\n",
    "        total_loss += loss.item()\n",
    "        n += 1\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    if epoch % PRINT_FREQ == 0:\n",
    "        print(total_loss / n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
